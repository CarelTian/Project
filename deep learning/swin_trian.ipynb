{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6790bdb-b8e2-4bfe-8b15-84edc882a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv \n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as album \n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91490cb3-d52d-44aa-80e9-df7c2db50709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.50s)\n",
      "creating index...\n",
      "index created!\n",
      "5303 1118 2308\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = \"data\"\n",
    "coco = COCO(f\"{DATASET_PATH}/annotations.json\")\n",
    "df=pd.read_csv(\"data/metadata_splits.csv\")\n",
    "train_ids,valid_ids,test_ids = [],[],[]\n",
    "for index, row in df.iterrows():\n",
    "    if row['split_open'] == 'train':\n",
    "        train_ids.append(row['id'])\n",
    "    elif row['split_open'] == 'valid':\n",
    "        valid_ids.append(row['id'])\n",
    "    elif row['split_open'] == 'test':\n",
    "        test_ids.append(row['id'])\n",
    "print(len(train_ids), len(valid_ids), len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a90da55-d4f2-4a52-81a1-2f73bc57852d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    transform = [\n",
    "        album.HorizontalFlip(p=0.5),\n",
    "        album.VerticalFlip(p=0.5),\n",
    "        album.RandomRotate90(p=0.5),\n",
    "    ]\n",
    "    return album.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "970ba1f2-3eab-4632-a439-fec021e265cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurtlesDataset(Dataset):\n",
    "    def __init__(self, coco, image_ids,augmentation=None):\n",
    "        self.coco = coco\n",
    "        self.image_ids = image_ids\n",
    "        self.augmentation = augmentation\n",
    "        self.catIds = coco.getCatIds()\n",
    "        self.label_priority = {\n",
    "            'head': 3,  \n",
    "            'flipper': 2,\n",
    "            'turtle': 1 \n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        image = self.coco.loadImgs([image_id])[0]\n",
    "        image = cv.imread(f\"{DATASET_PATH}/{image['file_name']}\")\n",
    "        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.int32)\n",
    "        priority_map = np.zeros_like(mask)  \n",
    "        \n",
    "        ann_ids = coco.getAnnIds(imgIds=image_id, catIds=self.catIds, iscrowd=None)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "        \n",
    "        for ann in anns:\n",
    "            category_id = ann['category_id']\n",
    "            category_name = coco.loadCats([category_id])[0]['name']\n",
    "            ann_mask = coco.annToMask(ann)\n",
    "            \n",
    "            current_priority = self.label_priority.get(category_name, 0)\n",
    "    \n",
    "            mask = np.where((ann_mask == 1) & (priority_map == 0), category_id, mask)\n",
    "            \n",
    "            mask = np.where((ann_mask == 1) & (current_priority > priority_map), category_id, mask)\n",
    "            \n",
    "            priority_map = np.where(ann_mask == 1, np.maximum(priority_map, current_priority), priority_map)\n",
    "        image = cv.resize(image, (512, 512))\n",
    "        mask = cv.resize(mask, (512, 512), interpolation=cv.INTER_NEAREST)\n",
    "        \n",
    "        image = image.astype(np.float32)\n",
    "        mask = mask.astype(np.float32)\n",
    "        if self.augmentation:\n",
    "            transformed = self.augmentation(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        transforms = [ToTensorV2()]     \n",
    "        composed = album.Compose(transforms)\n",
    "        transformed = composed(image=image, mask=mask)\n",
    "        image, mask=transformed['image'], transformed['mask']        \n",
    "        return image, mask\n",
    "\n",
    "train_dataset = TurtlesDataset(coco, train_ids, \n",
    "                         augmentation=get_augmentation())\n",
    "val_dataset = TurtlesDataset(coco, valid_ids,\n",
    "                            augmentation=get_augmentation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9bae542-04e8-4885-b9f4-182a93ffa158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiClassDiceFocalLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6, alpha=0.25, gamma=2.0, dice_weight=0.5, reduction='mean', apply_softmax=True):\n",
    "        super(MultiClassDiceFocalLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.dice_weight = dice_weight\n",
    "        self.reduction = reduction\n",
    "        self.apply_softmax = apply_softmax\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        device = inputs.device\n",
    "        num_classes = inputs.size(1)\n",
    "\n",
    "        # Apply softmax if specified\n",
    "        if self.apply_softmax:\n",
    "            inputs = torch.softmax(inputs, dim=1)\n",
    "\n",
    "        # Convert targets to one-hot encoding and ensure it's on the same device\n",
    "        targets_one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float().to(device)\n",
    "\n",
    "        # Ensure shapes match for inputs and targets_one_hot\n",
    "        if inputs.shape != targets_one_hot.shape:\n",
    "            raise ValueError(f\"Shape mismatch: inputs.shape = {inputs.shape}, targets_one_hot.shape = {targets_one_hot.shape}\")\n",
    "\n",
    "        # Dice Loss calculation\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(inputs * targets_one_hot, dims)\n",
    "        cardinality = torch.sum(inputs + targets_one_hot, dims)\n",
    "        dice_loss = 1 - (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "\n",
    "        # Calculate Dice Loss based on reduction type\n",
    "        if self.reduction == 'mean':\n",
    "            dice_loss = dice_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            dice_loss = dice_loss.sum()\n",
    "        \n",
    "        # Focal Loss calculation\n",
    "        ce_loss = -targets_one_hot * torch.log(inputs + 1e-8)  # Use a smaller epsilon for numerical stability\n",
    "        focal_loss = self.alpha * (1 - inputs) ** self.gamma * ce_loss\n",
    "        focal_loss = focal_loss.sum(dim=1)  # Sum over classes\n",
    "\n",
    "        # Apply reduction to focal loss\n",
    "        if self.reduction == 'mean':\n",
    "            focal_loss = focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            focal_loss = focal_loss.sum()\n",
    "\n",
    "        # Combine Dice and Focal Loss\n",
    "        combined_loss = self.dice_weight * dice_loss + (1 - self.dice_weight) * focal_loss\n",
    "        return combined_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1456f6e-bf19-403f-aa8b-b14d396b6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred, target, num_classes):\n",
    "\n",
    "    iou_per_class = []\n",
    "    for cls in range(1,num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds & target_inds).sum().item()\n",
    "        union = (pred_inds | target_inds).sum().item()\n",
    "        \n",
    "        if union == 0:\n",
    "            iou_per_class.append(float('nan')) \n",
    "        else:\n",
    "            iou_per_class.append(intersection / union)\n",
    "    \n",
    "    return iou_per_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd26cb25-cc8c-4e5d-a97c-2ba48170092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "\n",
    "class SwinSegmentation(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SwinSegmentation, self).__init__()\n",
    "        \n",
    "        # Create a Swin Transformer backbone model using timm\n",
    "        self.backbone = timm.create_model(\n",
    "            'swin_small_patch4_window7_224', \n",
    "            pretrained=True,\n",
    "            features_only=True, \n",
    "            img_size=512\n",
    "        )\n",
    "\n",
    "        # Create an FPN (Feature Pyramid Network) for multi-scale feature fusion\n",
    "        self.fpn = FeaturePyramidNetwork(\n",
    "            in_channels_list=[96, 192, 384, 768],  # Input channels from the backbone features\n",
    "            out_channels=64\n",
    "        )\n",
    "\n",
    "        # Convolution layer for classification (outputs num_classes channels)\n",
    "        self.classifier = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x) \n",
    "        # Convert features to (batch, channels, height, width) format\n",
    "        features = [f.permute(0, 3, 1, 2) for f in features]\n",
    "        feature_maps = {i: features[i] for i in range(len(features))}\n",
    "        \n",
    "        # Fuse multi-scale features using FPN\n",
    "        fpn_out = self.fpn(feature_maps)\n",
    "        out = self.classifier(fpn_out[0])  \n",
    "        # Upsample to the input image size\n",
    "        out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cce6cf4-ed2f-4f0c-b5f8-704c4d9bac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "device='mps'\n",
    "num_classes = 4 \n",
    "model = SwinSegmentation(num_classes)\n",
    "model.to(device)\n",
    "criterion = MultiClassDiceFocalLoss(smooth=1e-6, alpha=0.25, gamma=2.0, dice_weight=0)\n",
    "weights = [1.0, 1.3, 1.3, 1.3]\n",
    "#criterion=WeightedCrossEntropyLoss(weights)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=1e-5,betas=(0.9, 0.999))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a525c1-2a9e-4b05-bdd9-43d4ca1a2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "Epoch [1/25] - Training:   1%|    | 39/5303 [00:14<30:30,  2.88it/s, loss=0.327]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 25\n",
    "best_val_loss = float('inf')\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iou_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\", leave=False)\n",
    "    \n",
    "    for images, masks in train_loader_tqdm:\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        masks = masks.to(device, dtype=torch.long)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    iou_scores = []\n",
    "    val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader_tqdm:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            masks = masks.to(device, dtype=torch.long)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)  \n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            for i in range(preds.size(0)): \n",
    "                iou_per_class = calculate_iou(preds[i].cpu(), masks[i].cpu(), num_classes)\n",
    "                iou_scores.append(iou_per_class)\n",
    "            \n",
    "\n",
    "            val_loader_tqdm.set_postfix(val_loss=loss.item())\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    mean_iou = torch.tensor(iou_scores).nanmean(dim=0).tolist() \n",
    "    mean_iou_overall = torch.tensor(iou_scores).nanmean().item() \n",
    "    scheduler.step(avg_val_loss) \n",
    "    \n",
    "    val_losses.append(avg_val_loss)  \n",
    "    iou_per_epoch.append(mean_iou_overall)\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "  \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n",
    "    print(f'Class-wise IOU: {mean_iou}')\n",
    "    print(f'Mean IOU (Overall): {mean_iou_overall:.4f}')\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'NestedUnet_epoch_{epoch+1}.pth')\n",
    "        print(f\"Model saved at epoch {epoch+1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a4427-ea5e-4ac8-9217-dc53c0902964",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=[i+1 for i in range(num_epochs)]\n",
    "plt.figure(figsize=(20, 12))\n",
    "plt.plot(num_epochs, train_losses, label='Training Loss')\n",
    "plt.plot(num_epochs, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3760b31-1fb1-493e-bb6e-5cfca77a63fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "plt.plot(num_epochs, iou_per_epoch, label='Mean IoU', color='tab:blue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.title('Mean IoU over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa64aee-c2f7-42c0-8e2b-2ffa80cf4abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0df1ed-7977-424c-8699-cd438db72f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
